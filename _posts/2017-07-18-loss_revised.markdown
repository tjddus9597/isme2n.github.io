---
layout: post
title: "[project] loss 수정한 버전 설명"
subtitle:   "변형 loss"
categories: project
tags: project files
---

## wgt_loss version

### FTEmbed.lua

``` lua
function FTEmbedCriterion:updateOutput(input, gt_dist,gt_dist_norm)

    local wgt_mrg = self.mrg * (1 + (gt_dist_norm:repeatTensor(m, 1) - gt_dist_norm:repeatTensor(m, 1):t())):cuda()
    local loss = (dist:repeatTensor(m, 1):t() - dist:repeatTensor(m, 1)):add(wgt_mrg)

end
```
> gt_dist_norm(dist_smp_norm) : ({1,2 gt_distance}, {1,3 gt_distance}) ... 63개

```
 0.1258
 0.1293
 0.1316
 0.1350
 0.1371
 0.1781
 0.1866
 0.2251
 0.2301
 0.2303
 0.2400
 0.2432
 0.2441
 0.2447
 0.2450
 0.2463
 0.2479
 0.2501
 0.2536
 0.2545
 0.2608
 0.2658
 0.2711
 0.2733
 0.2734
 0.2762
 0.2772
 0.2790
 0.2817
 0.2826
 0.2853
 0.2894
 0.2915
 0.2946
 0.3096
 0.3143
 0.3263
 0.3275
 0.3300
 0.3366
 0.3418
 0.3473
 0.3554
 0.3587
 0.3779
 0.3807
 0.3832
 0.3837
 0.3871
 0.3876
 0.4222
 0.4249
 0.4295
 0.4321
 0.4375
 0.4386
 0.4753
 0.4795
 0.4945
 0.4960
 0.5159
 0.5317
 0.6653
 ```

wgt_mrg = mrg * (1 + w) <br>
w => <br>
$$
\begin{bmatrix}
gt(1,2)-gt(1,2) & gt(1,3)-gt(1,2) & gt(1,4)-gt(1,2)\\
gt(1,2)-gt(1,3) & gt(1,3)-gt(1,3) & gt(1,4)-gt(1,3)\\
gt(1,2)-gt(1,4) & gt(1,3)-gt(1,4) & \ddots
\end{bmatrix}
$$

```
0.0000  0.0034  0.0058  0.0092  0.0113  0.0350  0.0883  0.0913  0.0935  0.0945
-0.0034  0.0000  0.0024  0.0057  0.0078  0.0316  0.0848  0.0879  0.0901  0.0910
-0.0058 -0.0024  0.0000  0.0034  0.0055  0.0293  0.0825  0.0856  0.0878  0.0887
-0.0092 -0.0057 -0.0034  0.0000  0.0021  0.0259  0.0791  0.0822  0.0844  0.0853
-0.0113 -0.0078 -0.0055 -0.0021  0.0000  0.0238  0.0770  0.0801  0.0823  0.0832
-0.0350 -0.0316 -0.0293 -0.0259 -0.0238  0.0000  0.0532  0.0563  0.0585  0.0594
-0.0883 -0.0848 -0.0825 -0.0791 -0.0770 -0.0532  0.0000  0.0031  0.0053  0.0062
-0.0913 -0.0879 -0.0856 -0.0822 -0.0801 -0.0563 -0.0031  0.0000  0.0022  0.0031
-0.0935 -0.0901 -0.0878 -0.0844 -0.0823 -0.0585 -0.0053 -0.0022  0.0000  0.0009
-0.0945 -0.0910 -0.0887 -0.0853 -0.0832 -0.0594 -0.0062 -0.0031 -0.0009  0.0000
-0.1039 -0.1005 -0.0982 -0.0948 -0.0927 -0.0689 -0.0157 -0.0126 -0.0104 -0.0095
```
loss => <br>

$$
\begin{bmatrix}
d(1,2)-d(1,2) & d(1,2)-d(1,3) & d(1,2)-d(1,4)\\
d(1,3)-d(1,2) & d(1,3)-d(1,3) & d(1,3)-d(1,4)\\
d(1,4)-d(1,2) & d(1,4)-d(1,3) & \ddots
\end{bmatrix} + self.mrg(1+
\begin{bmatrix}
gt(1,2)-gt(1,2) & gt(1,3)-gt(1,2) & gt(1,4)-gt(1,2)\\
gt(1,2)-gt(1,3) & gt(1,3)-gt(1,3) & gt(1,4)-gt(1,3)\\
gt(1,2)-gt(1,4) & gt(1,3)-gt(1,4) & \ddots
\end{bmatrix})
$$

gradInput은 margin부분이 미분되면 사라지므로 기존의 loss와 같음